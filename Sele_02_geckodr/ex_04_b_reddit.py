import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys # Cáº§n thÃªm thÆ° viá»‡n nÃ y Ä‘á»ƒ dÃ¹ng Keys.ENTER

# --- Cáº¤U HÃŒNH ---
REDDIT_USERNAME = "FriendlyCharity9130"  
REDDIT_PASSWORD = "dinhquockhanh8888" 
TARGET_SUBREDDIT = "https://www.reddit.com/r/PokemonTGCP/" 
SCROLL_TIMES = 5 

def crawl_subreddit_data(driver, url):
    """Crawl dá»¯ liá»‡u dá»±a trÃªn phÃ¢n tÃ­ch tháº» <shreddit-post>"""
    print(f"ğŸš€ Äang truy cáº­p subreddit: {url}")
    driver.get(url)
    time.sleep(5) 

    # --- Ká»¸ THUáº¬T INFINITE SCROLL ---
    print(f"â¬‡ï¸ Báº¯t Ä‘áº§u cuá»™n trang {SCROLL_TIMES} láº§n...")
    for i in range(SCROLL_TIMES):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        print(f"   ÄÃ£ cuá»™n láº§n {i+1}/{SCROLL_TIMES} - Chá» load dá»¯ liá»‡u...")
        time.sleep(4) 

    # --- TRÃCH XUáº¤T Dá»® LIá»†U ---
    print("ğŸ” Äang quÃ©t cÃ¡c tháº» <shreddit-post>...")
    posts = driver.find_elements(By.TAG_NAME, "shreddit-post")
    
    data_list = []
    
    for post in posts:
        try:
            item = {
                'title': post.get_attribute("post-title"),
                'score': post.get_attribute("score"),
                'author': post.get_attribute("author"),
                'subreddit': post.get_attribute("subreddit-prefixed-name"),
                'created_at': post.get_attribute("created-timestamp"),
                'comment_count': post.get_attribute("comment-count"),
                'post_type': post.get_attribute("post-type"),
                'permalink': "https://www.reddit.com" + post.get_attribute("permalink") if post.get_attribute("permalink") else None
            }
            
            if item['author'] and item['title']:
                data_list.append(item)
                
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi parse má»™t post: {e}")
            continue

    return data_list

def save_to_csv(data):
    if not data:
        print("âŒ KhÃ´ng thu tháº­p Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o.")
        return

    df = pd.DataFrame(data)
    
    try:
        df['created_at'] = pd.to_datetime(df['created_at'])
    except:
        pass

    filename = "reddit_data.csv"
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… ÄÃ£ lÆ°u {len(df)} dÃ²ng dá»¯ liá»‡u vÃ o file '{filename}'")
    print(df.head())

# --- MAIN ---
if __name__ == "__main__":
    # 1. Khá»Ÿi táº¡o Driver cÆ¡ báº£n theo yÃªu cáº§u
    print("ğŸ”„ Khá»Ÿi táº¡o Chrome Driver...")
    driver = webdriver.Chrome()

    try:
        # 2. Quy trÃ¬nh Ä‘Äƒng nháº­p má»›i
        url_login = 'https://www.reddit.com/login/'
        print("ğŸ”‘ Äang truy cáº­p trang Ä‘Äƒng nháº­p...")
        driver.get(url_login)

        # Chá» Ã´ username xuáº¥t hiá»‡n
        email_element = WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.NAME, "username"))
        )

        print(f"âœï¸ Nháº­p username: {REDDIT_USERNAME}")
        email_element.send_keys(REDDIT_USERNAME)

        print("âœï¸ Nháº­p password...")
        pass_element = driver.find_element(By.NAME, "password")
        pass_element.send_keys(REDDIT_PASSWORD)
        
        print("ğŸ–±ï¸ Nháº¥n Enter Ä‘á»ƒ Ä‘Äƒng nháº­p...")
        pass_element.send_keys(Keys.ENTER)

        print("â³ Chá» 10s Ä‘á»ƒ chuyá»ƒn hÆ°á»›ng...")
        time.sleep(10)

        # 3. Tiáº¿n hÃ nh Crawl
        data = crawl_subreddit_data(driver, TARGET_SUBREDDIT)
        
        # 4. LÆ°u dá»¯ liá»‡u
        save_to_csv(data)
        
    except Exception as e:
        print(f"âŒ CÃ³ lá»—i xáº£y ra: {e}")
        
    finally:
        print("ğŸ›‘ ÄÃ³ng trÃ¬nh duyá»‡t.")
        driver.quit()